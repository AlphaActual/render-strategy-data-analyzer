{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cd1cdef",
   "metadata": {},
   "source": [
    "# üöÄ Comprehensive Multi-Page Performance Analysis\n",
    "\n",
    "## Advanced Core Web Vitals Analysis Across Multiple Pages\n",
    "\n",
    "This notebook analyzes lighthouse-generated Core Web Vitals data for **3 frameworks** (Next.js, Nuxt.js, SvelteKit) across **4 rendering strategies** (CSR, SSR, SSG, ISR) on **3 different pages** (Blog, About, BlogPost).\n",
    "\n",
    "### üìä Analysis Structure:\n",
    "- **Individual Page Analysis** - Detailed performance metrics for each page\n",
    "- **Cross-Page Comparison** - Compare performance patterns across different page types\n",
    "- **Structured Output** - Results saved in organized folders for each page\n",
    "- **Comprehensive Insights** - Best practices and recommendations per page type\n",
    "\n",
    "### üéØ Key Metrics Analyzed:\n",
    "- **First Contentful Paint (FCP)** - Time until first text/image appears\n",
    "- **Largest Contentful Paint (LCP)** - Time until largest content element loads\n",
    "- **Speed Index** - How quickly content is visually displayed\n",
    "- **Time to Interactive (TTI)** - Time until page becomes fully interactive\n",
    "- **Total Blocking Time (TBT)** - Time when main thread is blocked\n",
    "- **Cumulative Layout Shift (CLS)** - Visual stability metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure plotly for better display\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üìÅ Setting up directory structure...\")\n",
    "\n",
    "# Create output directory structure\n",
    "output_dir = Path('output')\n",
    "pages = ['blog', 'about', 'blogPost']\n",
    "\n",
    "for page in pages:\n",
    "    page_dir = output_dir / page\n",
    "    page_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "# Create comparison directory\n",
    "comparison_dir = output_dir / 'cross_page_comparison'\n",
    "comparison_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Directory structure created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cc6194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data files\n",
    "input_files = {\n",
    "    'blog': 'inputs/master_blog_comparison_slow4g_2025-06-10T09-41-32-118Z.csv',\n",
    "    'about': 'inputs/master_about_comparison_slow4g_2025-06-10T10-11-32-511Z.csv',\n",
    "    'blogPost': 'inputs/master_blogPost_comparison_slow4g_2025-06-10T10-38-29-481Z.csv'\n",
    "}\n",
    "\n",
    "# Load all datasets\n",
    "datasets = {}\n",
    "for page_name, file_path in input_files.items():\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['Page_Type'] = page_name  # Add page identifier\n",
    "        datasets[page_name] = df\n",
    "        print(f\"‚úÖ {page_name.upper()} data loaded: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {page_name}: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Total pages loaded: {len(datasets)}\")\n",
    "\n",
    "# Display sample from each dataset\n",
    "for page_name, df in datasets.items():\n",
    "    print(f\"\\nüîç {page_name.upper()} Sample:\")\n",
    "    print(f\"Frameworks: {df['Framework'].unique()}\")\n",
    "    print(f\"Strategies: {df['Strategy'].unique()}\")\n",
    "    print(f\"Total runs per test: {df['Total_Runs'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee3975",
   "metadata": {},
   "source": [
    "## üîß Data Processing & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c95252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and structure data\n",
    "def process_page_data(df, page_name):\n",
    "    \"\"\"Process raw lighthouse data into structured format\"\"\"\n",
    "    metrics_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        base_info = {\n",
    "            'Page_Type': page_name,\n",
    "            'App_Name': row['App_Name'],\n",
    "            'Framework': row['Framework'],\n",
    "            'Strategy': row['Strategy'],\n",
    "            'Total_Runs': row['Total_Runs']\n",
    "        }\n",
    "        \n",
    "        # Core Web Vitals metrics\n",
    "        metrics = {\n",
    "            'FCP': {'value': row['First Contentful Paint_Avg_Value'], 'score': row['First Contentful Paint_Avg_Score_%']},\n",
    "            'LCP': {'value': row['Largest Contentful Paint_Avg_Value'], 'score': row['Largest Contentful Paint_Avg_Score_%']},\n",
    "            'SI': {'value': row['Speed Index_Avg_Value'], 'score': row['Speed Index_Avg_Score_%']},\n",
    "            'TTI': {'value': row['Interactive_Avg_Value'], 'score': row['Interactive_Avg_Score_%']},\n",
    "            'TBT': {'value': row['Total Blocking Time_Avg_Value'], 'score': row['Total Blocking Time_Avg_Score_%']},\n",
    "            'CLS': {'value': row['Cumulative Layout Shift_Avg_Value'], 'score': row['Cumulative Layout Shift_Avg_Score_%']}\n",
    "        }\n",
    "        \n",
    "        for metric_name, metric_data in metrics.items():\n",
    "            metrics_data.append({\n",
    "                **base_info,\n",
    "                'Metric': metric_name,\n",
    "                'Value': metric_data['value'],\n",
    "                'Score': metric_data['score']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(metrics_data)\n",
    "\n",
    "# Process all datasets\n",
    "cleaned_datasets = {}\n",
    "for page_name, df in datasets.items():\n",
    "    cleaned_df = process_page_data(df, page_name)\n",
    "    cleaned_datasets[page_name] = cleaned_df\n",
    "    print(f\"‚úÖ {page_name.upper()} processed: {cleaned_df.shape}\")\n",
    "\n",
    "# Combine all data for cross-page analysis\n",
    "all_data = pd.concat(cleaned_datasets.values(), ignore_index=True)\n",
    "print(f\"\\nüîó Combined dataset shape: {all_data.shape}\")\n",
    "\n",
    "# Display sample of combined data\n",
    "print(\"\\nüìã Combined Data Sample:\")\n",
    "display(all_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d05ea6f",
   "metadata": {},
   "source": [
    "## üìä Individual Page Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57dccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze individual page performance\n",
    "def analyze_page_performance(df, page_name):\n",
    "    \"\"\"Comprehensive analysis for a single page\"\"\"\n",
    "    print(f\"\\nüîç ANALYZING {page_name.upper()} PAGE PERFORMANCE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create pivot tables\n",
    "    values_pivot = df.pivot_table(\n",
    "        index=['Framework', 'Strategy'], \n",
    "        columns='Metric', \n",
    "        values='Value', \n",
    "        aggfunc='mean'\n",
    "    ).round(3)\n",
    "    \n",
    "    scores_pivot = df.pivot_table(\n",
    "        index=['Framework', 'Strategy'], \n",
    "        columns='Metric', \n",
    "        values='Score', \n",
    "        aggfunc='mean'\n",
    "    ).round(1)\n",
    "    \n",
    "    # Calculate rankings\n",
    "    framework_rankings = df.groupby('Framework')['Score'].mean().sort_values(ascending=False)\n",
    "    strategy_rankings = df.groupby('Strategy')['Score'].mean().sort_values(ascending=False)\n",
    "    combination_rankings = df.groupby(['Framework', 'Strategy'])['Score'].mean().sort_values(ascending=False)\n",
    "    \n",
    "    # Best performers by metric\n",
    "    metric_leaders = {}\n",
    "    metrics_list = ['FCP', 'LCP', 'SI', 'TTI', 'TBT', 'CLS']\n",
    "    for metric in metrics_list:\n",
    "        metric_data = df[df['Metric'] == metric]\n",
    "        best = metric_data.loc[metric_data['Score'].idxmax()]\n",
    "        metric_leaders[metric] = {\n",
    "            'framework': best['Framework'],\n",
    "            'strategy': best['Strategy'],\n",
    "            'score': best['Score'],\n",
    "            'value': best['Value']\n",
    "        }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nüèÜ TOP PERFORMERS:\")\n",
    "    print(f\"Best Framework: {framework_rankings.index[0]} ({framework_rankings.iloc[0]:.1f}%)\")\n",
    "    print(f\"Best Strategy: {strategy_rankings.index[0]} ({strategy_rankings.iloc[0]:.1f}%)\")\n",
    "    print(f\"Best Combination: {combination_rankings.index[0][0]} + {combination_rankings.index[0][1]} ({combination_rankings.iloc[0]:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüéØ METRIC LEADERS:\")\n",
    "    for metric, leader in metric_leaders.items():\n",
    "        print(f\"{metric}: {leader['framework']} + {leader['strategy']} ({leader['score']:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'values_pivot': values_pivot,\n",
    "        'scores_pivot': scores_pivot,\n",
    "        'framework_rankings': framework_rankings,\n",
    "        'strategy_rankings': strategy_rankings,\n",
    "        'combination_rankings': combination_rankings,\n",
    "        'metric_leaders': metric_leaders\n",
    "    }\n",
    "\n",
    "# Analyze each page\n",
    "page_analyses = {}\n",
    "for page_name, df in cleaned_datasets.items():\n",
    "    analysis = analyze_page_performance(df, page_name)\n",
    "    page_analyses[page_name] = analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d814e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for each page\n",
    "def create_page_visualizations(df, page_name, analysis_results):\n",
    "    \"\"\"Create comprehensive visualizations for a single page\"\"\"\n",
    "    \n",
    "    # 1. Performance Heatmap\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Scores heatmap\n",
    "    sns.heatmap(analysis_results['scores_pivot'], annot=True, cmap='RdYlGn', center=75, \n",
    "               fmt='.1f', ax=ax1, cbar_kws={'label': 'Score (%)'}, linewidths=0.5)\n",
    "    ax1.set_title(f'üéØ {page_name.upper()} Performance Scores\\n(Higher is Better)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Core Web Vitals Metrics', fontweight='bold')\n",
    "    ax1.set_ylabel('Framework + Strategy', fontweight='bold')\n",
    "    \n",
    "    # Values heatmap\n",
    "    sns.heatmap(analysis_results['values_pivot'], annot=True, cmap='RdYlGn_r', \n",
    "               fmt='.2f', ax=ax2, cbar_kws={'label': 'Value (seconds/units)'}, linewidths=0.5)\n",
    "    ax2.set_title(f'‚è±Ô∏è {page_name.upper()} Performance Values\\n(Lower is Generally Better)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Core Web Vitals Metrics', fontweight='bold')\n",
    "    ax2.set_ylabel('Framework + Strategy', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'output/{page_name}/{page_name}_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Framework Performance Bar & Line Chart\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Overall Framework Performance (Bar Chart)\n",
    "    framework_scores = df.groupby('Framework')['Score'].mean().sort_values(ascending=False)\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    bars = ax1.bar(framework_scores.index, framework_scores.values, \n",
    "                  color=colors[:len(framework_scores)], alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    ax1.set_title(f'üèÜ {page_name.upper()} Overall Framework Performance', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Average Performance Score (%)', fontweight='bold')\n",
    "    ax1.set_ylim(0, 100)\n",
    "    ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, framework_scores.values):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{score:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    # Framework Performance by Metric (Line Chart)\n",
    "    metrics_list = ['FCP', 'LCP', 'SI', 'TTI', 'TBT', 'CLS']\n",
    "    framework_metric_data = []\n",
    "    \n",
    "    for framework in df['Framework'].unique():\n",
    "        fw_data = df[df['Framework'] == framework]\n",
    "        metric_scores = []\n",
    "        for metric in metrics_list:\n",
    "            metric_score = fw_data[fw_data['Metric'] == metric]['Score'].mean()\n",
    "            metric_scores.append(metric_score)\n",
    "        framework_metric_data.append((framework, metric_scores))\n",
    "    \n",
    "    # Plot lines for each framework\n",
    "    line_colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    markers = ['o', 's', '^']\n",
    "    \n",
    "    for i, (framework, scores) in enumerate(framework_metric_data):\n",
    "        ax2.plot(metrics_list, scores, \n",
    "                color=line_colors[i % len(line_colors)], \n",
    "                marker=markers[i % len(markers)], \n",
    "                linewidth=3, markersize=8, \n",
    "                label=framework, alpha=0.8)\n",
    "        \n",
    "        # Add value labels on points\n",
    "        for j, score in enumerate(scores):\n",
    "            ax2.annotate(f'{score:.0f}', \n",
    "                       (j, score), \n",
    "                       textcoords=\"offset points\", \n",
    "                       xytext=(0,10), \n",
    "                       ha='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax2.set_title(f'üìà {page_name.upper()} Performance by Metric', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Performance Score (%)', fontweight='bold')\n",
    "    ax2.set_xlabel('Core Web Vitals Metrics', fontweight='bold')\n",
    "    ax2.set_ylim(0, 105)\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax2.legend(title='Framework', loc='lower right', fontsize=10)\n",
    "    \n",
    "    plt.suptitle(f'üöÄ {page_name.upper()} Framework Performance Analysis', \n",
    "                fontsize=18, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'output/{page_name}/{page_name}_framework_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Interactive Radar Chart\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    frameworks = analysis_results['scores_pivot'].index.get_level_values('Framework').unique()\n",
    "    strategies = analysis_results['scores_pivot'].index.get_level_values('Strategy').unique()\n",
    "    \n",
    "    strategy_colors = {\n",
    "        'Client-Side Rendering': '#FF6B6B',\n",
    "        'Server-Side Rendering': '#4ECDC4', \n",
    "        'Static Site Generation': '#45B7D1',\n",
    "        'Incremental Static Regeneration': '#96CEB4'\n",
    "    }\n",
    "    \n",
    "    for framework in frameworks:\n",
    "        for strategy in strategies:\n",
    "            if (framework, strategy) in analysis_results['scores_pivot'].index:\n",
    "                values = analysis_results['scores_pivot'].loc[(framework, strategy)].tolist()\n",
    "                values.append(values[0])  # Close the radar chart\n",
    "                \n",
    "                fig.add_trace(go.Scatterpolar(\n",
    "                    r=values,\n",
    "                    theta=metrics_list + [metrics_list[0]],\n",
    "                    fill='toself',\n",
    "                    name=f'{framework} - {strategy}',\n",
    "                    line_color=strategy_colors.get(strategy, '#666666'),\n",
    "                    opacity=0.7\n",
    "                ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(visible=True, range=[0, 100], tickfont=dict(size=10)),\n",
    "            angularaxis=dict(tickfont=dict(size=12))\n",
    "        ),\n",
    "        showlegend=True,\n",
    "        title={\n",
    "            'text': f'üéØ {page_name.upper()} Performance Radar Chart<br><sub>Core Web Vitals Scores (0-100%)</sub>',\n",
    "            'x': 0.5,\n",
    "            'font': {'size': 16}\n",
    "        },\n",
    "        height=700\n",
    "    )\n",
    "    \n",
    "    fig.write_html(f'output/{page_name}/{page_name}_radar_chart.html')\n",
    "    fig.show()\n",
    "\n",
    "# Create visualizations for each page\n",
    "for page_name, df in cleaned_datasets.items():\n",
    "    print(f\"\\nüìä Creating visualizations for {page_name.upper()}...\")\n",
    "    create_page_visualizations(df, page_name, page_analyses[page_name])\n",
    "    print(f\"‚úÖ {page_name.upper()} visualizations saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7cd1f8",
   "metadata": {},
   "source": [
    "## üîÑ Cross-Page Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f49871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-page analysis\n",
    "print(\"üîÑ CROSS-PAGE PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Overall page performance comparison\n",
    "page_performance = all_data.groupby('Page_Type')['Score'].agg(['mean', 'std', 'min', 'max']).round(2)\n",
    "page_performance.columns = ['Average_Score', 'Std_Dev', 'Min_Score', 'Max_Score']\n",
    "page_performance = page_performance.sort_values('Average_Score', ascending=False)\n",
    "\n",
    "print(\"\\nüìä OVERALL PAGE PERFORMANCE RANKING:\")\n",
    "for i, (page, stats) in enumerate(page_performance.iterrows(), 1):\n",
    "    print(f\"{i}. {page.upper()}: {stats['Average_Score']:.1f}% (¬±{stats['Std_Dev']:.1f})\")\n",
    "\n",
    "display(page_performance)\n",
    "\n",
    "# Framework performance across pages\n",
    "framework_across_pages = all_data.groupby(['Page_Type', 'Framework'])['Score'].mean().unstack().round(1)\n",
    "print(\"\\nüèóÔ∏è FRAMEWORK PERFORMANCE ACROSS PAGES:\")\n",
    "display(framework_across_pages)\n",
    "\n",
    "# Strategy performance across pages\n",
    "strategy_across_pages = all_data.groupby(['Page_Type', 'Strategy'])['Score'].mean().unstack().round(1)\n",
    "print(\"\\nüîÑ STRATEGY PERFORMANCE ACROSS PAGES:\")\n",
    "display(strategy_across_pages)\n",
    "\n",
    "# Metric performance across pages\n",
    "metric_across_pages = all_data.groupby(['Page_Type', 'Metric'])['Score'].mean().unstack().round(1)\n",
    "print(\"\\nüéØ METRIC PERFORMANCE ACROSS PAGES:\")\n",
    "display(metric_across_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de8e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-page visualizations\n",
    "\n",
    "# 1. Page Performance Comparison\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 12))\n",
    "\n",
    "# Overall page scores\n",
    "page_scores = all_data.groupby('Page_Type')['Score'].mean().sort_values(ascending=False)\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "bars = ax1.bar(page_scores.index, page_scores.values, color=colors, alpha=0.8)\n",
    "ax1.set_title('üìä Overall Page Performance', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Average Score (%)')\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, score in zip(bars, page_scores.values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "            f'{score:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# Framework performance across pages\n",
    "framework_across_pages.plot(kind='bar', ax=ax2, color=['#FF9999', '#66B2FF', '#99FF99'])\n",
    "ax2.set_title('üèóÔ∏è Framework Performance Across Pages', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Average Score (%)')\n",
    "ax2.legend(title='Framework', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Strategy performance across pages\n",
    "strategy_across_pages.plot(kind='bar', ax=ax3)\n",
    "ax3.set_title('üîÑ Strategy Performance Across Pages', fontsize=14, fontweight='bold')\n",
    "ax3.set_ylabel('Average Score (%)')\n",
    "ax3.legend(title='Strategy', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Page consistency (standard deviation)\n",
    "page_consistency = all_data.groupby('Page_Type')['Score'].std().sort_values()\n",
    "bars = ax4.bar(page_consistency.index, page_consistency.values, color=colors, alpha=0.8)\n",
    "ax4.set_title('üìà Page Performance Consistency\\n(Lower = More Consistent)', fontsize=14, fontweight='bold')\n",
    "ax4.set_ylabel('Standard Deviation')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, std in zip(bars, page_consistency.values):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "            f'{std:.1f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/cross_page_comparison/cross_page_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. Detailed Cross-Page Heatmap\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "# Framework heatmap\n",
    "sns.heatmap(framework_across_pages, annot=True, cmap='RdYlGn', center=75, \n",
    "           fmt='.1f', ax=axes[0], cbar_kws={'label': 'Score (%)'}, linewidths=0.5)\n",
    "axes[0].set_title('üèóÔ∏è Framework Performance\\nAcross Pages', fontweight='bold')\n",
    "axes[0].set_xlabel('Framework')\n",
    "axes[0].set_ylabel('Page Type')\n",
    "\n",
    "# Strategy heatmap\n",
    "sns.heatmap(strategy_across_pages, annot=True, cmap='RdYlGn', center=75, \n",
    "           fmt='.1f', ax=axes[1], cbar_kws={'label': 'Score (%)'}, linewidths=0.5)\n",
    "axes[1].set_title('üîÑ Strategy Performance\\nAcross Pages', fontweight='bold')\n",
    "axes[1].set_xlabel('Strategy')\n",
    "axes[1].set_ylabel('Page Type')\n",
    "\n",
    "# Metric heatmap\n",
    "sns.heatmap(metric_across_pages, annot=True, cmap='RdYlGn', center=75, \n",
    "           fmt='.1f', ax=axes[2], cbar_kws={'label': 'Score (%)'}, linewidths=0.5)\n",
    "axes[2].set_title('üéØ Metric Performance\\nAcross Pages', fontweight='bold')\n",
    "axes[2].set_xlabel('Metric')\n",
    "axes[2].set_ylabel('Page Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/cross_page_comparison/cross_page_heatmaps.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0602f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive cross-page comparison chart\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Overall Page Performance', 'Framework Consistency Across Pages',\n",
    "                   'Strategy Effectiveness by Page', 'Metric Performance Distribution'),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "           [{\"type\": \"bar\"}, {\"type\": \"box\"}]]\n",
    ")\n",
    "\n",
    "# 1. Overall page performance\n",
    "page_scores = all_data.groupby('Page_Type')['Score'].mean().sort_values(ascending=False)\n",
    "fig.add_trace(\n",
    "    go.Bar(x=page_scores.index, y=page_scores.values, \n",
    "           name='Page Performance', marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1']),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Framework consistency scatter\n",
    "for framework in all_data['Framework'].unique():\n",
    "    fw_data = all_data[all_data['Framework'] == framework]\n",
    "    page_means = fw_data.groupby('Page_Type')['Score'].mean()\n",
    "    page_stds = fw_data.groupby('Page_Type')['Score'].std()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=page_means.values, y=page_stds.values,\n",
    "                  mode='markers+text', name=framework,\n",
    "                  text=page_means.index, textposition='top center',\n",
    "                  marker=dict(size=12, opacity=0.7)),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# 3. Strategy effectiveness\n",
    "strategy_page_scores = all_data.groupby(['Strategy', 'Page_Type'])['Score'].mean().unstack()\n",
    "for strategy in strategy_page_scores.index:\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=strategy_page_scores.columns, y=strategy_page_scores.loc[strategy],\n",
    "               name=strategy),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# 4. Metric distribution\n",
    "for page in all_data['Page_Type'].unique():\n",
    "    page_data = all_data[all_data['Page_Type'] == page]\n",
    "    fig.add_trace(\n",
    "        go.Box(y=page_data['Score'], name=page),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='üîÑ Comprehensive Cross-Page Performance Analysis',\n",
    "    height=800,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Page Type\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Average Score (%)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Mean Score\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Standard Deviation\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Page Type\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Score (%)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Score (%)\", row=2, col=2)\n",
    "\n",
    "fig.write_html('output/cross_page_comparison/interactive_cross_page_analysis.html')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be002fa0",
   "metadata": {},
   "source": [
    "## üíæ Export Results & Generate Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save individual page results\n",
    "def save_page_results(page_name, analysis_results, df):\n",
    "    \"\"\"Save comprehensive results for each page\"\"\"\n",
    "    output_path = Path(f'output/{page_name}')\n",
    "    \n",
    "    # Save pivot tables\n",
    "    analysis_results['values_pivot'].to_csv(output_path / f'{page_name}_performance_values.csv')\n",
    "    analysis_results['scores_pivot'].to_csv(output_path / f'{page_name}_performance_scores.csv')\n",
    "    \n",
    "    # Save rankings\n",
    "    analysis_results['framework_rankings'].to_csv(output_path / f'{page_name}_framework_rankings.csv')\n",
    "    analysis_results['strategy_rankings'].to_csv(output_path / f'{page_name}_strategy_rankings.csv')\n",
    "    analysis_results['combination_rankings'].to_csv(output_path / f'{page_name}_combination_rankings.csv')\n",
    "    \n",
    "    # Save detailed report\n",
    "    with open(output_path / f'{page_name}_performance_report.txt', 'w') as f:\n",
    "        f.write(f\"{page_name.upper()} PAGE PERFORMANCE REPORT\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"FRAMEWORK RANKINGS\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for i, (framework, score) in enumerate(analysis_results['framework_rankings'].items(), 1):\n",
    "            f.write(f\"{i}. {framework}: {score:.1f}%\\n\")\n",
    "        \n",
    "        f.write(\"\\nSTRATEGY RANKINGS\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for i, (strategy, score) in enumerate(analysis_results['strategy_rankings'].items(), 1):\n",
    "            f.write(f\"{i}. {strategy}: {score:.1f}%\\n\")\n",
    "        \n",
    "        f.write(\"\\nBEST COMBINATIONS\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for i, ((framework, strategy), score) in enumerate(analysis_results['combination_rankings'].head(5).items(), 1):\n",
    "            f.write(f\"{i}. {framework} + {strategy}: {score:.1f}%\\n\")\n",
    "        \n",
    "        f.write(\"\\nMETRIC LEADERS\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for metric, leader in analysis_results['metric_leaders'].items():\n",
    "            f.write(f\"{metric}: {leader['framework']} + {leader['strategy']} ({leader['score']:.1f}%, {leader['value']:.3f})\\n\")\n",
    "    \n",
    "    # Save raw cleaned data\n",
    "    df.to_csv(output_path / f'{page_name}_cleaned_data.csv', index=False)\n",
    "    \n",
    "    print(f\"‚úÖ {page_name.upper()} results saved to output/{page_name}/\")\n",
    "\n",
    "# Save results for each page\n",
    "for page_name in cleaned_datasets.keys():\n",
    "    save_page_results(page_name, page_analyses[page_name], cleaned_datasets[page_name])\n",
    "\n",
    "print(\"\\nüíæ SAVING CROSS-PAGE COMPARISON RESULTS...\")\n",
    "\n",
    "# Save cross-page analysis results\n",
    "comparison_path = Path('output/cross_page_comparison')\n",
    "\n",
    "# Save cross-page tables\n",
    "page_performance.to_csv(comparison_path / 'overall_page_performance.csv')\n",
    "framework_across_pages.to_csv(comparison_path / 'framework_across_pages.csv')\n",
    "strategy_across_pages.to_csv(comparison_path / 'strategy_across_pages.csv')\n",
    "metric_across_pages.to_csv(comparison_path / 'metric_across_pages.csv')\n",
    "\n",
    "# Save complete dataset\n",
    "all_data.to_csv(comparison_path / 'complete_dataset.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Cross-page comparison results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df6e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive final report\n",
    "print(\"üìã GENERATING COMPREHENSIVE FINAL REPORT...\")\n",
    "\n",
    "with open('output/COMPREHENSIVE_PERFORMANCE_REPORT.txt', 'w') as f:\n",
    "    f.write(\"üöÄ COMPREHENSIVE MULTI-PAGE PERFORMANCE ANALYSIS REPORT\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "    \n",
    "    # Executive Summary\n",
    "    f.write(\"üìä EXECUTIVE SUMMARY\\n\")\n",
    "    f.write(\"-\" * 20 + \"\\n\")\n",
    "    f.write(f\"Pages Analyzed: {len(cleaned_datasets)}\\n\")\n",
    "    f.write(f\"Frameworks Tested: {', '.join(all_data['Framework'].unique())}\\n\")\n",
    "    f.write(f\"Strategies Tested: {len(all_data['Strategy'].unique())}\\n\")\n",
    "    f.write(f\"Total Test Combinations: {len(all_data.groupby(['Page_Type', 'Framework', 'Strategy']))}\\n\")\n",
    "    f.write(f\"Overall Average Score: {all_data['Score'].mean():.1f}%\\n\\n\")\n",
    "    \n",
    "    # Page Rankings\n",
    "    f.write(\"üèÜ OVERALL PAGE PERFORMANCE RANKINGS\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    for i, (page, stats) in enumerate(page_performance.iterrows(), 1):\n",
    "        f.write(f\"{i}. {page.upper()}: {stats['Average_Score']:.1f}% (¬±{stats['Std_Dev']:.1f})\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    # Individual Page Analysis\n",
    "    for page_name, analysis in page_analyses.items():\n",
    "        f.write(f\"üìñ {page_name.upper()} PAGE ANALYSIS\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        f.write(f\"Best Framework: {analysis['framework_rankings'].index[0]} ({analysis['framework_rankings'].iloc[0]:.1f}%)\\n\")\n",
    "        f.write(f\"Best Strategy: {analysis['strategy_rankings'].index[0]} ({analysis['strategy_rankings'].iloc[0]:.1f}%)\\n\")\n",
    "        f.write(f\"Best Combination: {analysis['combination_rankings'].index[0][0]} + {analysis['combination_rankings'].index[0][1]} ({analysis['combination_rankings'].iloc[0]:.1f}%)\\n\")\n",
    "        \n",
    "        f.write(\"\\nMetric Leaders:\\n\")\n",
    "        for metric, leader in analysis['metric_leaders'].items():\n",
    "            f.write(f\"  ‚Ä¢ {metric}: {leader['framework']} + {leader['strategy']} ({leader['score']:.1f}%)\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    # Cross-Page Insights\n",
    "    f.write(\"üîÑ CROSS-PAGE INSIGHTS\\n\")\n",
    "    f.write(\"-\" * 25 + \"\\n\")\n",
    "    \n",
    "    # Best overall framework\n",
    "    overall_best_framework = all_data.groupby('Framework')['Score'].mean().sort_values(ascending=False)\n",
    "    f.write(f\"Best Overall Framework: {overall_best_framework.index[0]} ({overall_best_framework.iloc[0]:.1f}% avg across all pages)\\n\")\n",
    "    \n",
    "    # Best overall strategy\n",
    "    overall_best_strategy = all_data.groupby('Strategy')['Score'].mean().sort_values(ascending=False)\n",
    "    f.write(f\"Best Overall Strategy: {overall_best_strategy.index[0]} ({overall_best_strategy.iloc[0]:.1f}% avg across all pages)\\n\")\n",
    "    \n",
    "    # Most consistent performer\n",
    "    framework_consistency = all_data.groupby('Framework')['Score'].std().sort_values()\n",
    "    f.write(f\"Most Consistent Framework: {framework_consistency.index[0]} (œÉ={framework_consistency.iloc[0]:.1f})\\n\")\n",
    "    \n",
    "    strategy_consistency = all_data.groupby('Strategy')['Score'].std().sort_values()\n",
    "    f.write(f\"Most Consistent Strategy: {strategy_consistency.index[0]} (œÉ={strategy_consistency.iloc[0]:.1f})\\n\\n\")\n",
    "    \n",
    "    # Key Recommendations\n",
    "    f.write(\"üí° KEY RECOMMENDATIONS\\n\")\n",
    "    f.write(\"-\" * 25 + \"\\n\")\n",
    "    f.write(f\"1. FOR MAXIMUM PERFORMANCE: Use {overall_best_framework.index[0]} with {overall_best_strategy.index[0]}\\n\")\n",
    "    f.write(f\"2. FOR CONSISTENCY: Choose {framework_consistency.index[0]} framework with {strategy_consistency.index[0]} strategy\\n\")\n",
    "    f.write(f\"3. PAGE-SPECIFIC OPTIMIZATION: Focus on {page_performance.index[-1]} page (lowest performer)\\n\")\n",
    "    f.write(f\"4. METRIC PRIORITIES: Address TBT and LCP metrics across all pages\\n\")\n",
    "    f.write(f\"5. TESTING APPROACH: Validate performance gains with A/B testing\\n\\n\")\n",
    "    \n",
    "    # Performance Patterns\n",
    "    f.write(\"üìà PERFORMANCE PATTERNS\\n\")\n",
    "    f.write(\"-\" * 25 + \"\\n\")\n",
    "    \n",
    "    # Check if SSG/SSR outperforms CSR\n",
    "    csr_score = all_data[all_data['Strategy'] == 'Client-Side Rendering']['Score'].mean()\n",
    "    ssg_score = all_data[all_data['Strategy'] == 'Static Site Generation']['Score'].mean()\n",
    "    if ssg_score > csr_score:\n",
    "        f.write(f\"‚Ä¢ Static rendering strategies outperform client-side rendering ({ssg_score:.1f}% vs {csr_score:.1f}%)\\n\")\n",
    "    \n",
    "    # Framework performance gap\n",
    "    fw_gap = overall_best_framework.iloc[0] - overall_best_framework.iloc[-1]\n",
    "    if fw_gap > 10:\n",
    "        f.write(f\"‚Ä¢ Significant framework performance differences detected ({fw_gap:.1f}% gap)\\n\")\n",
    "    else:\n",
    "        f.write(f\"‚Ä¢ Framework performance is relatively similar (within {fw_gap:.1f}%)\\n\")\n",
    "    \n",
    "    # Page complexity insights\n",
    "    page_complexity = page_performance['Std_Dev'].sort_values(ascending=False)\n",
    "    f.write(f\"‚Ä¢ {page_complexity.index[0]} page shows highest performance variability (œÉ={page_complexity.iloc[0]:.1f})\\n\")\n",
    "    f.write(f\"‚Ä¢ {page_complexity.index[-1]} page shows most consistent performance (œÉ={page_complexity.iloc[-1]:.1f})\\n\\n\")\n",
    "    \n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(\"üìÅ GENERATED FILES:\\n\")\n",
    "    f.write(\"Individual page results: output/[page_name]/\\n\")\n",
    "    f.write(\"Cross-page comparison: output/cross_page_comparison/\\n\")\n",
    "    f.write(\"Complete dataset: output/cross_page_comparison/complete_dataset.csv\\n\")\n",
    "    f.write(\"Interactive charts: *.html files in respective folders\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Comprehensive report generated: COMPREHENSIVE_PERFORMANCE_REPORT.txt\")\n",
    "\n",
    "# Generate summary statistics\n",
    "summary_stats = {\n",
    "    'total_pages': len(cleaned_datasets),\n",
    "    'total_frameworks': len(all_data['Framework'].unique()),\n",
    "    'total_strategies': len(all_data['Strategy'].unique()),\n",
    "    'total_combinations': len(all_data.groupby(['Framework', 'Strategy'])),\n",
    "    'overall_average_score': all_data['Score'].mean(),\n",
    "    'best_page': page_performance.index[0],\n",
    "    'best_framework': overall_best_framework.index[0],\n",
    "    'best_strategy': overall_best_strategy.index[0],\n",
    "    'analysis_timestamp': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('output/analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(\"\\nüéâ ANALYSIS COMPLETE!\")\n",
    "print(\"üìÅ All results saved to the output/ directory\")\n",
    "print(\"üìä Check individual page folders for detailed analysis\")\n",
    "print(\"üîÑ Check cross_page_comparison/ for comparative insights\")\n",
    "print(\"üìã Read COMPREHENSIVE_PERFORMANCE_REPORT.txt for executive summary\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
